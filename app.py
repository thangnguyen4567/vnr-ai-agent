import streamlit as st
from src.core.multi_agent import multi_agent_graph as graph
import asyncio
from typing import AsyncGenerator
from src.core.config_loader import agent_config_loader
from langfuse.langchain import CallbackHandler
from src.config import settings
from langfuse import Langfuse
import streamlit.components.v1 as components

# Thi·∫øt l·∫≠p ti√™u ƒë·ªÅ ·ª©ng d·ª•ng
st.set_page_config(page_title="AI Chatbot", page_icon="ü§ñ")
st.title("ü§ñ AI Chatbot")

# Thi·∫øt l·∫≠p lo·∫°i agent l√† multi_agent
agent_config_loader.set_agent_type("multi")

# Kh·ªüi t·∫°o chat history trong session state n·∫øu ch∆∞a c√≥
if "messages" not in st.session_state:
    st.session_state.messages = []

# Kh·ªüi t·∫°o config trong session state n·∫øu ch∆∞a c√≥
if "config" not in st.session_state:
    Langfuse(
        public_key=settings.LANGFUSE_CONFIG["public_key"],
        secret_key=settings.LANGFUSE_CONFIG["secret_key"],
        host=settings.LANGFUSE_CONFIG["host"],
    )
    langfuse_handler = CallbackHandler()
    st.session_state.config = {
        "configurable": {
            "current_date": "06/06/2025",
            "language": "vi-VN",
            "agent_id": "",
            "thread_id": "113",
            "user_info": {
                "name": "Nguy·ªÖn VƒÉn A",
                "employee_id": "EMP001"
            }
        },
        "callbacks": [langfuse_handler],
        "recursion_limit": 10,
    }


async def process_message():
    full_response = ""
    async for event in graph.astream_events(inputs, config=st.session_state.config):
        kind = event["event"]

        if kind == "on_custom_event":
            thinking_content = event["data"]["data"]["text"]
            yield thinking_content

        elif kind == "on_chat_model_stream" and event["metadata"].get(
            "langgraph_node"
        ) not in ["research", "reflection", "router_agent"]:
            answer_content = event["data"]["chunk"].content
            if answer_content:
                full_response += answer_content
                yield answer_content
    
    # L∆∞u tin nh·∫Øn c·ªßa bot v√†o l·ªãch s·ª≠ sau khi x·ª≠ l√Ω xong
    if full_response:
        st.session_state.messages.append({"role": "assistant", "content": full_response})


def to_sync_generator(async_gen: AsyncGenerator):
    # 1. T·∫°o m·ªõi m·ªôt loop
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    try:
        while True:
            try:
                # Ch·∫°y 1 b∆∞·ªõc c·ªßa async gen
                yield loop.run_until_complete(async_gen.__anext__())
            except StopAsyncIteration:
                break
    finally:
        # H·ªßy loop khi xong
        loop.close()
        # Tr·∫£ l·∫°i setting ban ƒë·∫ßu (n·∫øu c·∫ßn)
        asyncio.set_event_loop(None)

# H√†m ƒë·ªÉ t·ª± ƒë·ªông cu·ªôn xu·ªëng cu·ªëi trang
def auto_scroll_to_bottom():
    js = '''
    <script>
        function scroll_to_bottom() {
            window.scrollTo(0, document.body.scrollHeight);
        }
        scroll_to_bottom();
    </script>
    '''
    components.html(js, height=0)

# Hi·ªÉn th·ªã tin nh·∫Øn tr∆∞·ªõc ƒë√≥
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"], unsafe_allow_html=True)

# Giao di·ªán chat
if prompt := st.chat_input("Nh·∫≠p tin nh·∫Øn c·ªßa b·∫°n..."):
    # Th√™m tin nh·∫Øn ng∆∞·ªùi d√πng v√†o l·ªãch s·ª≠
    st.session_state.messages.append({"role": "user", "content": prompt})

    # Hi·ªÉn th·ªã tin nh·∫Øn ng∆∞·ªùi d√πng
    with st.chat_message("user"):
        st.markdown(prompt)

    # Hi·ªÉn th·ªã ƒëang x·ª≠ l√Ω
    with st.chat_message("assistant"):
        inputs = {"messages": [("user", prompt)]}
        response = st.write_stream(to_sync_generator(process_message()))
        
    # T·ª± ƒë·ªông cu·ªôn xu·ªëng cu·ªëi trang sau khi c√≥ ph·∫£n h·ªìi
    auto_scroll_to_bottom()
